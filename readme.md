## Cloning This Repository

This project uses [Git Large File Storage (Git LFS)](https://git-lfs.com/) to manage files larger than 100 MB.  
If you clone the repository without Git LFS, large files will appear only as tiny pointer text files.

### One-time setup
Install Git LFS on your system:

- **macOS (Homebrew):**
  ```bash
  brew install git-lfs
  ```
- **Ubuntu / Debian:**
  ```bash
  sudo apt-get install git-lfs
  ```
- **Windows:**
  Download the installer from [git-lfs.github.com](https://git-lfs.com/).

Then initialize it:
```bash
git lfs install
```

### Clone and fetch large files
```bash
git clone https://github.com/OWNER/REPO.git
cd REPO
git lfs pull
```

After this, all large files tracked by LFS (e.g., datasets, binaries, or models) will be downloaded automatically.

> ⚠️ **Note:** If you skip the `git lfs install` step, the large files will not download—you’ll only see placeholder text files.

# Project Structure

```
project/
├─ README.md
├─ LICENSE
├─ .gitignore
├─ CITATION.cff
├─ requirements.txt
├─ setup.sh                        # sets up the local Python env and installs deps
├─ analysis.ipynb                  # main analysis notebook that produces manuscript figures
├─ data/
│  ├─ llama_classified/
│  │  ├─ <batch_id>_environmentalism/   # individual responses from env item classification (joblib files)
│  │  └─ <batch_id>_feminism/           # individual responses from fem item classification (joblib files)
│  ├─ classification_environmentalism.xlsx   # clustered items and aggregated results (environmentalism)
│  ├─ classification_feminism.xlsx           # clustered items and aggregated results (feminism)
│  ├─ Systematic_review.xlsx                 # ORIGINAL INPUT data
│  ├─ items_df.pkl                           # dataset with precomputed embeddings, loaded by analysis.ipynb
├─ figures/                                  # output figures used in the manuscript
├─ modules/
│  ├─ __init__.py
│  ├─ utils.py
│  ├─ deduplication.py
│  ├─ similarity.py
│  ├─ embeddings.py
│  ├─ network.py
│  ├─ evolution.py
│  ├─ clustering.py
│  ├─ building_taxonomy.py
│  └─ tracking_subclasses.py
```


---
# Pipeline description

## Analysis Pipeline

1. **Input Data**  
   - Start from `data/Systematic_review.xlsx` (original coded items).

2. **Preprocessing & Deduplication**  
   - Done inside `analysis.ipynb` using helpers in `modules/utils.py` and `modules/deduplication.py`.  
   - Produces a clean set of unique items.

3. **Embeddings (manual, external step)**  
   - Generate embeddings and save to `data/items_df.pkl` (already pre-computed).  
   - This file is then loaded by `analysis.ipynb`.
   - New computation of empeddings can be done following instructions below (section embeddings)

4. **Clustering & Visualization**  
   - Performed in `analysis.ipynb` with `modules/clustering.py`, `modules/evolution.py` (Fig. S7), and `modules/network.py`.
   - Produces exploratory figures in `figures/`.

5. **Classification (manual, external step)**  
   - Run LLM-based classification script separately; outputs are stored under `data/llama_classified/<batch_id>_<domain>/`.  
   - Results are merged into aggregated tables:  
     - `data/classification_feminism.xlsx`  
     - `data/classification_environmentalism.xlsx`

6. **Taxonomy Building & Prevalence Tracking**  
   - `analysis.ipynb` merges classifications with clusters (`modules/building_taxonomy.py`) and propagates labels.  
   - Theme prevalence over time is computed with `modules/tracking_subclasses.py`.

7. **Figures**  
   - All manuscript figures (clustering plots, networks, heatmaps, prevalence trends) are generated by `analysis.ipynb` and stored in `figures/`.


# Project Setup

This project has been run on MacBooks with Apple Silicon (M2/M3/M4) and **≥16 GB** unified memory. The project consists of three components:

1) **Analysis pipeline: reproduction of results** — Orchestrated via `analysis.ipynb`, which reads the input dataset `Systematic_review.xlsx` the precomputed embeddings (`data/items_df.pkl`), loads classification outputs under `data/llama_classified/`, and produces the final **figures/** used in the manuscript.

2) **Embedding extraction** — Uses the **Qwen3-Embedding-8B** model (local via Ollama) to generate text embeddings. We ship **precomputed** embeddings in `data/items_df.pkl` so the notebook runs end-to-end without recomputing.

3) **Text classification** — Uses **Llama-3.1-70B** via a **managed KISSKI API (OpenAI-compatible)**. We do **not** self-host 70B locally; results are stored as per-item joblib files under `data/llama_classified/<batch_id>_{environmentalism|feminism}/` and aggregated into `data/classification_environmentalism_v1.xlsx` and `data/classification_feminism_v1.xslx` for expert manual inspection.

> **Disclaimer**  
> Llama-3.1-70B cannot be run meaningfully on a 16 GB MacBook; we therefore rely on an external provider endpoint for classification, as part of Germany's public infrastructure for research (https://kisski.gwdg.de/en/). Given the simplicity of the task, we expect similar results to be achieve with smaller models as Llama-3.1-8B that could be run locally using ollama, equivalently to what we have done with Qwen for embeddings, although this setup we haven't tested.

---

## Python Environment

We recommend Python 3.12.

1. Make the setup script executable and run it:
```bash
chmod +x setup.sh
./setup.sh
```

2. Activate the virtual environment (created by `setup.sh`):
```bash
source .venv/bin/activate
```
If you prefer manual installation:

```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

Unzip the classification data archive:

```bash
unzip data/llama_classified.zip -d data/
```

---

## Reproduce Results: Running the Analysis Notebook

Open the notebook and run all cells:

```bash
# From the repo root:
source .venv/bin/activate
jupyter lab  # or: jupyter notebook
# open analysis.ipynb and Run → Run All
```

The notebook will:
- Load `data/items_df.pkl` (precomputed embeddings).
- Load classification outputs from `data/llama_classified/*`.
- Produce all figures into `figures/`.

> **Headless run (optional):**
> ```bash
> jupyter nbconvert --to notebook --execute analysis.ipynb --output analysis_exec.ipynb
> ```

### Expected output

In the folder `figures/expected_outputs` we can find 

### Estimated execution time
The process of running the pipeline as-is (pre-computed embeddings and classification) is matter of a few minutes.

New embedding computation will take a 1-3 hours, depending on your system requirements.
New classification of all items will take between 1-N hours depending on your system specs. In our case, served by an external service (KISSKI - see below) took around 5 hours, since resources are allocated on demand for multiple users.

---

# Embeddings

This project uses the **Qwen3-Embedding-8B** model for embeddings. The file **`data/items_df.pkl`** already contains precomputed embeddings for both the original item texts and tokenized variants.

## Reproducing Embeddings (optional)

We recommend [Ollama](https://ollama.com/) for local deployment:

1) Install Ollama  
   Follow the installer for your platform.

2) Pull the model  
```bash
ollama pull qwen3-embedding:8b
```

3) (Optional) Test run  
```bash
ollama run qwen3-embedding:8b
```

4) Regenerate embeddings with the provided helper:
```bash
python modules/embeddings.py \
  --input data/Systematic_review.xlsx \
  --output data/items_df.pkl
```

**Notes**
- Ensure Ollama is running before executing the script.
- Adjust input/output paths as needed.
- The notebook expects `data/items_df.pkl`; regenerating will overwrite it.

---


# Text Classification — Llama‑3.1‑70B (KISSKI)

We reproduced classification results with **Llama‑3.1‑70B** served via a **KISSKI‑hosted, OpenAI‑compatible API**. We did **not** self‑host 70B locally.

- **Topics:** `environmentalism` and `feminism`.  
- **Per‑item outputs:**  
  - `data/llama_classified/<batch_id>_environmentalism/response_<index>.joblib`  
  - `data/llama_classified/<batch_id>_feminism/response_<index>.joblib`  
- **Aggregates:**  
  - `data/classification_environmentalism.xlsx`  
  - `data/classification_feminism.xlsx`

> **Why KISSKI?** 70B models do not fit or perform reliably on typical laptops (e.g., 16 GB RAM). Managed hosting improves reproducibility and avoids hardware pitfalls.

## Using the KISSKI Endpoint

Ask the project admin for the endpoint and credentials, then set:
```bash
export LLM_BASE_URL="https://<your-kisski-endpoint>/v1"
export LLM_API_KEY="<your_api_key>"
```

**Minimal cURL (Chat Completions):**
```bash
curl -X POST "$LLM_BASE_URL/chat/completions" \
  -H "Authorization: Bearer $LLM_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama-3.1-70b-instruct",
    "messages": [
      {"role":"system","content":"<PROMPT WITH <SEED> PLACEHOLDER>"},
      {"role":"user","content":"Classify: <SEED>"}
    ],
    "temperature": 0.0,
    "max_tokens": 128,
    "seed": 89
  }'
```

## Batch Runner We Used

We ship `run_batch_local_or_api.py`, which reads the topic CSV, applies the topic prompt, calls the backend, and writes `response_<index>.joblib` files.

- **Inputs & prompts**  
  - CSVs:  
    - `data/revision_sistematica__environmentalism__clustered_v1.csv`  
    - `data/revision_sistematica__feminism__clustered_v1.csv`  
  - Prompts (imported in the script):  
    - `modules.prompts.prompt_env.prompt_env`  
    - `modules.prompts.prompt_fem.prompt_fem`
- **Backends:** `kisski` (recommended), `ollama`, `vllm`  
- **Text column:** `"Item content"`

**Example (KISSKI):**
```bash
export LLM_BASE_URL="https://<your-kisski-endpoint>/v1"
export LLM_API_KEY="<your_api_key>"

# Store outputs under data/llama_classified/<batch_id>_feminism/
python run_batch_local_or_api.py \
  --backend kisski \
  --topic feminism \
  --batch-id llama_classified/20250713_feminism_kisski \
  --batch-size 50 \
  --temperature 0.7 \
  --model meta-llama-3.1-70b-instruct
```
This produces:
```
data/llama_classified/20250713_feminism_kisski/response_0.joblib
data/llama_classified/20250713_feminism_kisski/response_1.joblib
...
```

> **Resuming:** Re-running with the same `--batch-id` skips indices that already have `response_<index>.joblib`.

**Path sanity checks if you extend the runner:**
- `CSV_BY_TOPIC` should point to `data/...` (not `..data/...`).  
- When composing `--batch-id`, do **not** prefix with `data/`; the script already places outputs under `data/`. Use `llama_classified/<batch_id>_<topic>`.

---

## Reproducibility & Determinism

- **Decoding:** `temperature=0.0` (deterministic) and `seed=89` where supported.  
- **Recordkeeping:** capture **model id/tag**, **prompt version**, **batch_id**, and **run date**.  
- **Data integrity:** `data/Systematic_review.xlsx` is canonical; all derived artifacts live in `data/` and `figures/`.

---

## Data & Privacy

- Input data: `data/Systematic_review.xlsx`.  
- No personal data (PII) are included.  
- All intermediate artifacts remain within `data/` and `figures/`.

---

## Citing This Work

See `CITATION.cff` at the repository root (GitHub shows a “Cite this repository” button).

---

## License

See `LICENSE`. Please respect the license terms for code and artifacts.

---

## Troubleshooting

- **401/403 (KISSKI):** Verify `LLM_API_KEY` and access to the deployment.  
- **“File not found”:** Confirm paths under `data/llama_classified/` and topic CSV locations.  
- **Inconsistent outputs:** Ensure identical prompts/chat format; keep `temperature=0.0` for reproducibility.

---

## Changelog (high level)

- **v1.0** — Initial release with precomputed embeddings, KISSKI-based classification results, and manuscript figures.
